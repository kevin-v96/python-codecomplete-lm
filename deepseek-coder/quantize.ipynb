{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 23479, done.\u001b[K\n",
      "remote: Counting objects: 100% (9431/9431), done.\u001b[K\n",
      "remote: Compressing objects: 100% (550/550), done.\u001b[K\n",
      "remote: Total 23479 (delta 9208), reused 8884 (delta 8881), pack-reused 14048\u001b[K\n",
      "Receiving objects: 100% (23479/23479), 34.95 MiB | 29.00 MiB/s, done.\n",
      "Resolving deltas: 100% (16704/16704), done.\n",
      "Already up to date.\n",
      "I ccache not found. Consider installing it for faster compilation.\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE \n",
      "I NVCCFLAGS: -std=c++11 -O3 \n",
      "I LDFLAGS:    \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "rm -vrf *.o tests/*.o *.so *.a *.dll benchmark-matmult lookup-create lookup-merge lookup-stats common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity imatrix embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf gguf-split eval-callback llama-bench libllava.a llava-cli baby-llama beam-search retrieval speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup passkey gritlm tests/test-c.o tests/test-autorelease tests/test-backend-ops tests/test-double-float tests/test-grad0 tests/test-grammar-integration tests/test-grammar-parser tests/test-json-schema-to-grammar tests/test-llama-grammar tests/test-model-load-cancel tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-rope tests/test-sampling tests/test-tokenizer-0 tests/test-tokenizer-1-bpe tests/test-tokenizer-1-spm\n",
      "rm -vrf ggml-cuda/*.o\n",
      "find examples pocs -type f -name \"*.o\" -delete\n",
      "I ccache not found. Consider installing it for faster compilation.\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include \n",
      "I NVCCFLAGS: -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
      "I LDFLAGS:   -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I NVCC:      Build cuda_12.1.r12.1/compiler.32688072_0\n",
      "\n",
      "!!!!\n",
      "LLAMA_CUBLAS is deprecated and will be removed in the future. Use LLAMA_CUDA instead.\n",
      "!!!!\n",
      "\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c llama.cpp -o llama.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/common.cpp -o common.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/sampling.cpp -o sampling.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/grammar-parser.cpp -o grammar-parser.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/build-info.cpp -o build-info.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/json-schema-to-grammar.cpp -o json-schema-to-grammar.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/console.cpp -o console.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c sgemm.cpp -o sgemm.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda.cu -o ggml-cuda.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/acc.cu -o ggml-cuda/acc.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/alibi.cu -o ggml-cuda/alibi.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/arange.cu -o ggml-cuda/arange.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/argsort.cu -o ggml-cuda/argsort.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/binbcast.cu -o ggml-cuda/binbcast.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/clamp.cu -o ggml-cuda/clamp.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/concat.cu -o ggml-cuda/concat.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/convert.cu -o ggml-cuda/convert.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/cpy.cu -o ggml-cuda/cpy.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/diagmask.cu -o ggml-cuda/diagmask.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/dmmv.cu -o ggml-cuda/dmmv.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/getrows.cu -o ggml-cuda/getrows.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/im2col.cu -o ggml-cuda/im2col.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/mmq.cu -o ggml-cuda/mmq.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/mmvq.cu -o ggml-cuda/mmvq.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/norm.cu -o ggml-cuda/norm.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/pad.cu -o ggml-cuda/pad.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/pool2d.cu -o ggml-cuda/pool2d.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/quantize.cu -o ggml-cuda/quantize.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/rope.cu -o ggml-cuda/rope.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/scale.cu -o ggml-cuda/scale.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/softmax.cu -o ggml-cuda/softmax.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/sumrows.cu -o ggml-cuda/sumrows.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/tsembd.cu -o ggml-cuda/tsembd.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/unary.cu -o ggml-cuda/unary.o\n",
      "nvcc -std=c++11 -O3 -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -Xcompiler \"-std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Wno-pedantic\" -c ggml-cuda/upscale.cu -o ggml-cuda/upscale.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c unicode.cpp -o unicode.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c unicode-data.cpp -o unicode-data.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/main/main.cpp -o examples/main/main.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/main/main.o -o main -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize/quantize.o -o quantize -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o llama.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize-stats/quantize-stats.o -o quantize-stats -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/perplexity/perplexity.o -o perplexity -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/imatrix/imatrix.o -o imatrix -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/embedding/embedding.o -o embedding -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/vdot.o -o vdot -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/q8dot.o -o q8dot -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/train.cpp -o train.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/simple/simple.cpp -o examples/simple/simple.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/simple/simple.o -o simple -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/batched/batched.cpp -o examples/batched/batched.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched/batched.o -o batched -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o llama.o common.o sampling.o grammar-parser.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched-bench/batched-bench.o -o batched-bench -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/save-load-state/save-load-state.o -o save-load-state -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/server/server.cpp -o examples/server/server.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o -Iexamples/server examples/server/server.o -o server -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib  \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf/gguf.o -o gguf -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/gguf-split/gguf-split.cpp -o examples/gguf-split/gguf-split.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf-split/gguf-split.o -o gguf-split -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/eval-callback/eval-callback.cpp -o examples/eval-callback/eval-callback.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/eval-callback/eval-callback.o -o eval-callback -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llama-bench/llama-bench.o -o llama-bench -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/llava/llava.cpp -o examples/llava/llava.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/baby-llama/baby-llama.o -o baby-llama -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/beam-search/beam-search.o -o beam-search -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/retrieval/retrieval.cpp -o examples/retrieval/retrieval.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/retrieval/retrieval.o -o retrieval -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/speculative/speculative.o -o speculative -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/infill/infill.cpp -o examples/infill/infill.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/infill/infill.o -o infill -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/tokenize/tokenize.o -o tokenize -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  build-info.o ggml.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/parallel/parallel.o -o parallel -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/finetune/finetune.o -o finetune -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/export-lora/export-lora.o -o export-lora -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookahead/lookahead.o -o lookahead -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c common/ngram-cache.cpp -o ngram-cache.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup.o -o lookup -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/lookup/lookup-create.cpp -o examples/lookup/lookup-create.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-create.o -o lookup-create -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/lookup/lookup-merge.cpp -o examples/lookup/lookup-merge.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-merge.o -o lookup-merge -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/lookup/lookup-stats.cpp -o examples/lookup/lookup-stats.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-stats.o -o lookup-stats -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/passkey/passkey.o -o passkey -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-cuda.o ggml-cuda/acc.o ggml-cuda/alibi.o ggml-cuda/arange.o ggml-cuda/argsort.o ggml-cuda/binbcast.o ggml-cuda/clamp.o ggml-cuda/concat.o ggml-cuda/convert.o ggml-cuda/cpy.o ggml-cuda/diagmask.o ggml-cuda/dmmv.o ggml-cuda/getrows.o ggml-cuda/im2col.o ggml-cuda/mmq.o ggml-cuda/mmvq.o ggml-cuda/norm.o ggml-cuda/pad.o ggml-cuda/pool2d.o ggml-cuda/quantize.o ggml-cuda/rope.o ggml-cuda/scale.o ggml-cuda/softmax.o ggml-cuda/sumrows.o ggml-cuda/tsembd.o ggml-cuda/unary.o ggml-cuda/upscale.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gritlm/gritlm.o -o gritlm -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/usr/lib64 -L/usr/local/cuda/targets/x86_64-linux/lib -L/usr/lib/wsl/lib \n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_LLAMAFILE -DGGML_USE_CUDA -I/usr/local/cuda/include -I/usr/local/cuda/targets/x86_64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
      "Collecting numpy~=1.24.4 (from -r llama.cpp/./requirements/requirements-convert.txt (line 1))\n",
      "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting sentencepiece~=0.1.98 (from -r llama.cpp/./requirements/requirements-convert.txt (line 2))\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting transformers<5.0.0,>=4.35.2 (from -r llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 4))\n",
      "  Downloading gguf-0.6.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 5))\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting torch~=2.1.1 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting einops~=0.7.0 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3))\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3))\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)\n",
      "Collecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)\n",
      "Collecting triton==2.1.0 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
      "Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m166.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m187.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m228.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.1/774.1 kB\u001b[0m \u001b[31m171.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m192.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m219.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece, triton, tqdm, safetensors, regex, protobuf, nvidia-nccl-cu12, numpy, einops, huggingface-hub, gguf, torch, tokenizers, transformers\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
      "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0\n",
      "    Uninstalling torch-2.2.0:\n",
      "      Successfully uninstalled torch-2.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.0 requires torch==2.2.0, but you have torch 2.1.2 which is incompatible.\n",
      "torchvision 0.17.0 requires torch==2.2.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed einops-0.7.0 gguf-0.6.0 huggingface-hub-0.22.2 numpy-1.24.4 nvidia-nccl-cu12-2.18.1 protobuf-4.25.3 regex-2024.4.28 safetensors-0.4.3 sentencepiece-0.1.99 tokenizers-0.19.1 torch-2.1.2 tqdm-4.66.2 transformers-4.40.1 triton-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install llama.cpp\n",
    "! git clone https://github.com/ggerganov/llama.cpp\n",
    "! cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
    "! pip install -r llama.cpp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected operating system as Ubuntu/jammy.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Detected apt version as 2.4.11\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... done.\n",
      "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
      "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
      "done.\n",
      "Running apt-get update... done.\n",
      "\n",
      "The repository is setup! You can now install packages.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 55 not upgraded.\n",
      "Need to get 7420 kB of archives.\n",
      "After this operation, 16.5 MB of additional disk space will be used.\n",
      "Get:1 https://packagecloud.io/github/git-lfs/ubuntu jammy/main amd64 git-lfs amd64 3.5.1 [7420 kB]\n",
      "Fetched 7420 kB in 0s (16.6 MB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 21042 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_3.5.1_amd64.deb ...\n",
      "Unpacking git-lfs (3.5.1) ...\n",
      "Setting up git-lfs (3.5.1) ...\n",
      "Git LFS initialized.\n",
      "Git LFS initialized.\n",
      "Cloning into 'deepseek-coder-1.3b-python-peft'...\n",
      "remote: Enumerating objects: 20, done.\u001b[K\n",
      "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 20 (delta 4), reused 0 (delta 0), pack-reused 3\u001b[K\n",
      "Unpacking objects: 100% (20/20), 398.06 KiB | 3.46 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"MadMarx37/deepseek-coder-1.3b-python-peft\"\n",
    "\n",
    "# Download model\n",
    "! curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\n",
    "! apt-get install git-lfs\n",
    "#the two commands above (bash and apt-get) might need sudo based on where you're running them\n",
    "! git lfs install\n",
    "! git clone https://huggingface.co/{MODEL_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file deepseek-coder-1.3b-python-peft/model.safetensors\n",
      "params = Params(n_vocab=32256, n_embd=2048, n_layer=24, n_ctx=16384, n_ff=5504, n_head=16, n_head_kv=16, n_experts=None, n_experts_used=None, f_norm_eps=1e-06, rope_scaling_type=<RopeScalingType.LINEAR: 'linear'>, f_rope_freq_base=100000, f_rope_scale=4.0, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('deepseek-coder-1.3b-python-peft'))\n",
      "Loaded vocab file PosixPath('deepseek-coder-1.3b-python-peft/tokenizer.json'), type 'bpe'\n",
      "Vocab info: <BpeVocab with 32000 base tokens and 22 added tokens>\n",
      "Special vocab info: <SpecialVocab with 31757 merges, special tokens {'bos': 32013, 'eos': 32014, 'pad': 32014}, add special tokens {'bos': True, 'eos': False}>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "lm_head.weight                                   -> output.weight                            | F16    | [32256, 2048]\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32256, 2048]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [2048, 5504]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [5504, 2048]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [5504, 2048]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [2048, 2048]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [2048, 2048]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [2048, 5504]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [5504, 2048]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [5504, 2048]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [2048, 2048]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [2048, 2048]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [2048, 5504]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [5504, 2048]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [5504, 2048]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [2048, 2048]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [2048, 2048]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [2048]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [2048, 5504]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [5504, 2048]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [5504, 2048]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [2048]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [2048, 2048]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [2048, 2048]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [2048, 2048]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [2048, 2048]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [2048, 5504]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [5504, 2048]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [5504, 2048]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [2048, 2048]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [2048, 2048]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [2048, 5504]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [5504, 2048]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [5504, 2048]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [2048, 2048]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [2048, 2048]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [2048, 5504]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [5504, 2048]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [5504, 2048]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [2048, 2048]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [2048, 2048]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [2048, 5504]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [5504, 2048]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [5504, 2048]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [2048, 2048]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [2048, 2048]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [2048, 5504]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [5504, 2048]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [5504, 2048]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [2048, 2048]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [2048, 2048]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [2048, 5504]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [5504, 2048]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [5504, 2048]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [2048, 2048]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [2048, 2048]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [2048]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [2048, 5504]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [5504, 2048]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [5504, 2048]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [2048]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [2048, 2048]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [2048, 2048]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [2048, 2048]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [2048, 2048]\n",
      "model.norm.weight                                -> output_norm.weight                       | F16    | [2048]\n",
      "Writing deepseek-coder-1.3b-python-peft/deepseek-coder-1.3b-python-peft.fp16.bin, format 1\n",
      "Padding vocab with 234 token(s) - <dummy00001> through <dummy00234>\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 31757 merge(s).\n",
      "gguf: Setting special token type bos to 32013\n",
      "gguf: Setting special token type eos to 32014\n",
      "gguf: Setting special token type pad to 32014\n",
      "gguf: Setting add_bos_token to True\n",
      "gguf: Setting add_eos_token to False\n",
      "[  1/219] Writing tensor output.weight                          | size  32256 x   2048  | type F16  | T+   0\n",
      "[  2/219] Writing tensor token_embd.weight                      | size  32256 x   2048  | type F16  | T+   0\n",
      "[  3/219] Writing tensor blk.0.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[  4/219] Writing tensor blk.0.ffn_down.weight                  | size   2048 x   5504  | type F16  | T+   0\n",
      "[  5/219] Writing tensor blk.0.ffn_gate.weight                  | size   5504 x   2048  | type F16  | T+   0\n",
      "[  6/219] Writing tensor blk.0.ffn_up.weight                    | size   5504 x   2048  | type F16  | T+   0\n",
      "[  7/219] Writing tensor blk.0.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[  8/219] Writing tensor blk.0.attn_k.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[  9/219] Writing tensor blk.0.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 10/219] Writing tensor blk.0.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 11/219] Writing tensor blk.0.attn_v.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 12/219] Writing tensor blk.1.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 13/219] Writing tensor blk.1.ffn_down.weight                  | size   2048 x   5504  | type F16  | T+   0\n",
      "[ 14/219] Writing tensor blk.1.ffn_gate.weight                  | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 15/219] Writing tensor blk.1.ffn_up.weight                    | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 16/219] Writing tensor blk.1.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[ 17/219] Writing tensor blk.1.attn_k.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 18/219] Writing tensor blk.1.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 19/219] Writing tensor blk.1.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 20/219] Writing tensor blk.1.attn_v.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 21/219] Writing tensor blk.10.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 22/219] Writing tensor blk.10.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[ 23/219] Writing tensor blk.10.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 24/219] Writing tensor blk.10.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 25/219] Writing tensor blk.10.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 26/219] Writing tensor blk.10.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 27/219] Writing tensor blk.10.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 28/219] Writing tensor blk.10.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 29/219] Writing tensor blk.10.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 30/219] Writing tensor blk.11.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 31/219] Writing tensor blk.11.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[ 32/219] Writing tensor blk.11.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 33/219] Writing tensor blk.11.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 34/219] Writing tensor blk.11.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 35/219] Writing tensor blk.11.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 36/219] Writing tensor blk.11.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 37/219] Writing tensor blk.11.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 38/219] Writing tensor blk.11.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 39/219] Writing tensor blk.12.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 40/219] Writing tensor blk.12.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[ 41/219] Writing tensor blk.12.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 42/219] Writing tensor blk.12.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 43/219] Writing tensor blk.12.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 44/219] Writing tensor blk.12.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 45/219] Writing tensor blk.12.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 46/219] Writing tensor blk.12.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 47/219] Writing tensor blk.12.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 48/219] Writing tensor blk.13.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 49/219] Writing tensor blk.13.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[ 50/219] Writing tensor blk.13.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 51/219] Writing tensor blk.13.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 52/219] Writing tensor blk.13.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 53/219] Writing tensor blk.13.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 54/219] Writing tensor blk.13.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 55/219] Writing tensor blk.13.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 56/219] Writing tensor blk.13.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 57/219] Writing tensor blk.14.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 58/219] Writing tensor blk.14.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[ 59/219] Writing tensor blk.14.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 60/219] Writing tensor blk.14.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 61/219] Writing tensor blk.14.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 62/219] Writing tensor blk.14.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 63/219] Writing tensor blk.14.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 64/219] Writing tensor blk.14.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 65/219] Writing tensor blk.14.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 66/219] Writing tensor blk.15.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 67/219] Writing tensor blk.15.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[ 68/219] Writing tensor blk.15.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 69/219] Writing tensor blk.15.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 70/219] Writing tensor blk.15.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 71/219] Writing tensor blk.15.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 72/219] Writing tensor blk.15.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 73/219] Writing tensor blk.15.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 74/219] Writing tensor blk.15.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 75/219] Writing tensor blk.16.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 76/219] Writing tensor blk.16.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[ 77/219] Writing tensor blk.16.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 78/219] Writing tensor blk.16.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 79/219] Writing tensor blk.16.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 80/219] Writing tensor blk.16.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 81/219] Writing tensor blk.16.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 82/219] Writing tensor blk.16.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 83/219] Writing tensor blk.16.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 84/219] Writing tensor blk.17.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 85/219] Writing tensor blk.17.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[ 86/219] Writing tensor blk.17.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 87/219] Writing tensor blk.17.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 88/219] Writing tensor blk.17.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 89/219] Writing tensor blk.17.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 90/219] Writing tensor blk.17.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 91/219] Writing tensor blk.17.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 92/219] Writing tensor blk.17.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 93/219] Writing tensor blk.18.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[ 94/219] Writing tensor blk.18.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[ 95/219] Writing tensor blk.18.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 96/219] Writing tensor blk.18.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[ 97/219] Writing tensor blk.18.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[ 98/219] Writing tensor blk.18.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[ 99/219] Writing tensor blk.18.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[100/219] Writing tensor blk.18.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[101/219] Writing tensor blk.18.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[102/219] Writing tensor blk.19.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[103/219] Writing tensor blk.19.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[104/219] Writing tensor blk.19.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[105/219] Writing tensor blk.19.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[106/219] Writing tensor blk.19.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[107/219] Writing tensor blk.19.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[108/219] Writing tensor blk.19.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[109/219] Writing tensor blk.19.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[110/219] Writing tensor blk.19.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[111/219] Writing tensor blk.2.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[112/219] Writing tensor blk.2.ffn_down.weight                  | size   2048 x   5504  | type F16  | T+   0\n",
      "[113/219] Writing tensor blk.2.ffn_gate.weight                  | size   5504 x   2048  | type F16  | T+   0\n",
      "[114/219] Writing tensor blk.2.ffn_up.weight                    | size   5504 x   2048  | type F16  | T+   0\n",
      "[115/219] Writing tensor blk.2.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[116/219] Writing tensor blk.2.attn_k.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[117/219] Writing tensor blk.2.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[118/219] Writing tensor blk.2.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[119/219] Writing tensor blk.2.attn_v.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[120/219] Writing tensor blk.20.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[121/219] Writing tensor blk.20.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[122/219] Writing tensor blk.20.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[123/219] Writing tensor blk.20.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[124/219] Writing tensor blk.20.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[125/219] Writing tensor blk.20.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[126/219] Writing tensor blk.20.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[127/219] Writing tensor blk.20.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[128/219] Writing tensor blk.20.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[129/219] Writing tensor blk.21.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[130/219] Writing tensor blk.21.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[131/219] Writing tensor blk.21.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[132/219] Writing tensor blk.21.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[133/219] Writing tensor blk.21.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[134/219] Writing tensor blk.21.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[135/219] Writing tensor blk.21.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[136/219] Writing tensor blk.21.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[137/219] Writing tensor blk.21.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[138/219] Writing tensor blk.22.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[139/219] Writing tensor blk.22.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[140/219] Writing tensor blk.22.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[141/219] Writing tensor blk.22.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[142/219] Writing tensor blk.22.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[143/219] Writing tensor blk.22.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[144/219] Writing tensor blk.22.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[145/219] Writing tensor blk.22.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[146/219] Writing tensor blk.22.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[147/219] Writing tensor blk.23.attn_norm.weight                | size   2048           | type F32  | T+   0\n",
      "[148/219] Writing tensor blk.23.ffn_down.weight                 | size   2048 x   5504  | type F16  | T+   0\n",
      "[149/219] Writing tensor blk.23.ffn_gate.weight                 | size   5504 x   2048  | type F16  | T+   0\n",
      "[150/219] Writing tensor blk.23.ffn_up.weight                   | size   5504 x   2048  | type F16  | T+   0\n",
      "[151/219] Writing tensor blk.23.ffn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[152/219] Writing tensor blk.23.attn_k.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[153/219] Writing tensor blk.23.attn_output.weight              | size   2048 x   2048  | type F16  | T+   0\n",
      "[154/219] Writing tensor blk.23.attn_q.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[155/219] Writing tensor blk.23.attn_v.weight                   | size   2048 x   2048  | type F16  | T+   0\n",
      "[156/219] Writing tensor blk.3.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[157/219] Writing tensor blk.3.ffn_down.weight                  | size   2048 x   5504  | type F16  | T+   0\n",
      "[158/219] Writing tensor blk.3.ffn_gate.weight                  | size   5504 x   2048  | type F16  | T+   0\n",
      "[159/219] Writing tensor blk.3.ffn_up.weight                    | size   5504 x   2048  | type F16  | T+   0\n",
      "[160/219] Writing tensor blk.3.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[161/219] Writing tensor blk.3.attn_k.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[162/219] Writing tensor blk.3.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[163/219] Writing tensor blk.3.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[164/219] Writing tensor blk.3.attn_v.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[165/219] Writing tensor blk.4.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[166/219] Writing tensor blk.4.ffn_down.weight                  | size   2048 x   5504  | type F16  | T+   0\n",
      "[167/219] Writing tensor blk.4.ffn_gate.weight                  | size   5504 x   2048  | type F16  | T+   0\n",
      "[168/219] Writing tensor blk.4.ffn_up.weight                    | size   5504 x   2048  | type F16  | T+   0\n",
      "[169/219] Writing tensor blk.4.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[170/219] Writing tensor blk.4.attn_k.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[171/219] Writing tensor blk.4.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[172/219] Writing tensor blk.4.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[173/219] Writing tensor blk.4.attn_v.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[174/219] Writing tensor blk.5.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[175/219] Writing tensor blk.5.ffn_down.weight                  | size   2048 x   5504  | type F16  | T+   0\n",
      "[176/219] Writing tensor blk.5.ffn_gate.weight                  | size   5504 x   2048  | type F16  | T+   0\n",
      "[177/219] Writing tensor blk.5.ffn_up.weight                    | size   5504 x   2048  | type F16  | T+   0\n",
      "[178/219] Writing tensor blk.5.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[179/219] Writing tensor blk.5.attn_k.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[180/219] Writing tensor blk.5.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[181/219] Writing tensor blk.5.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[182/219] Writing tensor blk.5.attn_v.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[183/219] Writing tensor blk.6.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[184/219] Writing tensor blk.6.ffn_down.weight                  | size   2048 x   5504  | type F16  | T+   0\n",
      "[185/219] Writing tensor blk.6.ffn_gate.weight                  | size   5504 x   2048  | type F16  | T+   0\n",
      "[186/219] Writing tensor blk.6.ffn_up.weight                    | size   5504 x   2048  | type F16  | T+   0\n",
      "[187/219] Writing tensor blk.6.ffn_norm.weight                  | size   2048           | type F32  | T+   0\n",
      "[188/219] Writing tensor blk.6.attn_k.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[189/219] Writing tensor blk.6.attn_output.weight               | size   2048 x   2048  | type F16  | T+   0\n",
      "[190/219] Writing tensor blk.6.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[191/219] Writing tensor blk.6.attn_v.weight                    | size   2048 x   2048  | type F16  | T+   0\n",
      "[192/219] Writing tensor blk.7.attn_norm.weight                 | size   2048           | type F32  | T+   0\n",
      "[193/219] Writing tensor blk.7.ffn_down.weight                  | size   2048 x   5504  | type F16  | T+   0\n",
      "[194/219] Writing tensor blk.7.ffn_gate.weight                  | size   5504 x   2048  | type F16  | T+   0\n",
      "[195/219] Writing tensor blk.7.ffn_up.weight                    | size   5504 x   2048  | type F16  | T+   1\n",
      "[196/219] Writing tensor blk.7.ffn_norm.weight                  | size   2048           | type F32  | T+   1\n",
      "[197/219] Writing tensor blk.7.attn_k.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[198/219] Writing tensor blk.7.attn_output.weight               | size   2048 x   2048  | type F16  | T+   1\n",
      "[199/219] Writing tensor blk.7.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[200/219] Writing tensor blk.7.attn_v.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[201/219] Writing tensor blk.8.attn_norm.weight                 | size   2048           | type F32  | T+   1\n",
      "[202/219] Writing tensor blk.8.ffn_down.weight                  | size   2048 x   5504  | type F16  | T+   1\n",
      "[203/219] Writing tensor blk.8.ffn_gate.weight                  | size   5504 x   2048  | type F16  | T+   1\n",
      "[204/219] Writing tensor blk.8.ffn_up.weight                    | size   5504 x   2048  | type F16  | T+   1\n",
      "[205/219] Writing tensor blk.8.ffn_norm.weight                  | size   2048           | type F32  | T+   1\n",
      "[206/219] Writing tensor blk.8.attn_k.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[207/219] Writing tensor blk.8.attn_output.weight               | size   2048 x   2048  | type F16  | T+   1\n",
      "[208/219] Writing tensor blk.8.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[209/219] Writing tensor blk.8.attn_v.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[210/219] Writing tensor blk.9.attn_norm.weight                 | size   2048           | type F32  | T+   1\n",
      "[211/219] Writing tensor blk.9.ffn_down.weight                  | size   2048 x   5504  | type F16  | T+   1\n",
      "[212/219] Writing tensor blk.9.ffn_gate.weight                  | size   5504 x   2048  | type F16  | T+   1\n",
      "[213/219] Writing tensor blk.9.ffn_up.weight                    | size   5504 x   2048  | type F16  | T+   1\n",
      "[214/219] Writing tensor blk.9.ffn_norm.weight                  | size   2048           | type F32  | T+   1\n",
      "[215/219] Writing tensor blk.9.attn_k.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[216/219] Writing tensor blk.9.attn_output.weight               | size   2048 x   2048  | type F16  | T+   1\n",
      "[217/219] Writing tensor blk.9.attn_q.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[218/219] Writing tensor blk.9.attn_v.weight                    | size   2048 x   2048  | type F16  | T+   1\n",
      "[219/219] Writing tensor output_norm.weight                     | size   2048           | type F32  | T+   1\n",
      "Wrote deepseek-coder-1.3b-python-peft/deepseek-coder-1.3b-python-peft.fp16.bin\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
    "\n",
    "# Convert to fp16\n",
    "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
    "\n",
    "#vocab-type option is required for deepseek-coder models (https://github.com/ggerganov/llama.cpp/issues/6690#issuecomment-2058098140)\n",
    "! python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16} --vocab-type bpe --pad-vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 2770 (952d03db)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'deepseek-coder-1.3b-python-peft/deepseek-coder-1.3b-python-peft.fp16.bin' to 'deepseek-coder-1.3b-python-peft/deepseek-coder-1.3b-python-peft.Q5_K_M.gguf' as Q5_K_M\n",
      "llama_model_loader: loaded meta data with 25 key-value pairs and 219 tensors from deepseek-coder-1.3b-python-peft/deepseek-coder-1.3b-python-peft.fp16.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5504\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 100000.000000\n",
      "llama_model_loader: - kv  12:                    llama.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  13:                  llama.rope.scaling.factor f32              = 4.000000\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32256]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32256]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,31757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 32013\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 32014\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32014\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type  f16:  170 tensors\n",
      "[   1/ 219]                        output.weight - [ 2048, 32256,     1,     1], type =    f16, converting to q6_K .. size =   126.00 MiB ->    51.68 MiB\n",
      "[   2/ 219]                    token_embd.weight - [ 2048, 32256,     1,     1], type =    f16, converting to q5_K .. size =   126.00 MiB ->    43.31 MiB\n",
      "[   3/ 219]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   4/ 219]                blk.0.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[   5/ 219]                blk.0.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[   6/ 219]                  blk.0.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[   7/ 219]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[   8/ 219]                  blk.0.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[   9/ 219]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  10/ 219]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  11/ 219]                  blk.0.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  12/ 219]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  13/ 219]                blk.1.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[  14/ 219]                blk.1.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  15/ 219]                  blk.1.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  16/ 219]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  17/ 219]                  blk.1.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  18/ 219]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  19/ 219]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  20/ 219]                  blk.1.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  21/ 219]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  22/ 219]               blk.10.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[  23/ 219]               blk.10.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  24/ 219]                 blk.10.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  25/ 219]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  26/ 219]                 blk.10.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  27/ 219]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  28/ 219]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  29/ 219]                 blk.10.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  30/ 219]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  31/ 219]               blk.11.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[  32/ 219]               blk.11.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  33/ 219]                 blk.11.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  34/ 219]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  35/ 219]                 blk.11.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  36/ 219]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  37/ 219]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  38/ 219]                 blk.11.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  39/ 219]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  40/ 219]               blk.12.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[  41/ 219]               blk.12.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  42/ 219]                 blk.12.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  43/ 219]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  44/ 219]                 blk.12.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  45/ 219]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  46/ 219]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  47/ 219]                 blk.12.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  48/ 219]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  49/ 219]               blk.13.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[  50/ 219]               blk.13.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  51/ 219]                 blk.13.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  52/ 219]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  53/ 219]                 blk.13.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  54/ 219]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  55/ 219]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  56/ 219]                 blk.13.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  57/ 219]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  58/ 219]               blk.14.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[  59/ 219]               blk.14.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  60/ 219]                 blk.14.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  61/ 219]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  62/ 219]                 blk.14.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  63/ 219]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  64/ 219]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  65/ 219]                 blk.14.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  66/ 219]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  67/ 219]               blk.15.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[  68/ 219]               blk.15.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  69/ 219]                 blk.15.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  70/ 219]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  71/ 219]                 blk.15.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  72/ 219]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  73/ 219]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  74/ 219]                 blk.15.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  75/ 219]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  76/ 219]               blk.16.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[  77/ 219]               blk.16.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  78/ 219]                 blk.16.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  79/ 219]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  80/ 219]                 blk.16.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  81/ 219]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  82/ 219]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  83/ 219]                 blk.16.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  84/ 219]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  85/ 219]               blk.17.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[  86/ 219]               blk.17.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  87/ 219]                 blk.17.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  88/ 219]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  89/ 219]                 blk.17.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  90/ 219]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  91/ 219]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  92/ 219]                 blk.17.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  93/ 219]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  94/ 219]               blk.18.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[  95/ 219]               blk.18.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  96/ 219]                 blk.18.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[  97/ 219]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[  98/ 219]                 blk.18.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  99/ 219]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 100/ 219]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 101/ 219]                 blk.18.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 102/ 219]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 103/ 219]               blk.19.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[ 104/ 219]               blk.19.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 105/ 219]                 blk.19.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 106/ 219]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 107/ 219]                 blk.19.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 108/ 219]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 109/ 219]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 110/ 219]                 blk.19.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 111/ 219]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 112/ 219]                blk.2.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[ 113/ 219]                blk.2.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 114/ 219]                  blk.2.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 115/ 219]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 116/ 219]                  blk.2.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 117/ 219]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 118/ 219]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 119/ 219]                  blk.2.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 120/ 219]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 121/ 219]               blk.20.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[ 122/ 219]               blk.20.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 123/ 219]                 blk.20.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 124/ 219]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 125/ 219]                 blk.20.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 126/ 219]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 127/ 219]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 128/ 219]                 blk.20.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 129/ 219]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 130/ 219]               blk.21.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[ 131/ 219]               blk.21.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 132/ 219]                 blk.21.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 133/ 219]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 134/ 219]                 blk.21.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 135/ 219]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 136/ 219]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 137/ 219]                 blk.21.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 138/ 219]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 139/ 219]               blk.22.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[ 140/ 219]               blk.22.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 141/ 219]                 blk.22.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 142/ 219]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 143/ 219]                 blk.22.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 144/ 219]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 145/ 219]                 blk.22.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 146/ 219]                 blk.22.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 147/ 219]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 148/ 219]               blk.23.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[ 149/ 219]               blk.23.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 150/ 219]                 blk.23.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 151/ 219]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 152/ 219]                 blk.23.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 153/ 219]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 154/ 219]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 155/ 219]                 blk.23.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 156/ 219]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 157/ 219]                blk.3.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[ 158/ 219]                blk.3.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 159/ 219]                  blk.3.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 160/ 219]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 161/ 219]                  blk.3.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 162/ 219]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 163/ 219]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 164/ 219]                  blk.3.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 165/ 219]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 166/ 219]                blk.4.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[ 167/ 219]                blk.4.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 168/ 219]                  blk.4.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 169/ 219]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 170/ 219]                  blk.4.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 171/ 219]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 172/ 219]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 173/ 219]                  blk.4.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 174/ 219]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 175/ 219]                blk.5.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q5_K - using fallback quantization q5_1\n",
      "converting to q5_1 .. size =    21.50 MiB ->     8.06 MiB\n",
      "[ 176/ 219]                blk.5.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 177/ 219]                  blk.5.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 178/ 219]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 179/ 219]                  blk.5.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 180/ 219]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 181/ 219]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 182/ 219]                  blk.5.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 183/ 219]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 184/ 219]                blk.6.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[ 185/ 219]                blk.6.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 186/ 219]                  blk.6.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 187/ 219]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 188/ 219]                  blk.6.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 189/ 219]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 190/ 219]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 191/ 219]                  blk.6.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 192/ 219]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 193/ 219]                blk.7.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[ 194/ 219]                blk.7.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 195/ 219]                  blk.7.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 196/ 219]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 197/ 219]                  blk.7.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 198/ 219]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 199/ 219]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 200/ 219]                  blk.7.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 201/ 219]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 202/ 219]                blk.8.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[ 203/ 219]                blk.8.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 204/ 219]                  blk.8.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 205/ 219]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 206/ 219]                  blk.8.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 207/ 219]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 208/ 219]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 209/ 219]                  blk.8.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 210/ 219]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 211/ 219]                blk.9.ffn_down.weight - [ 5504,  2048,     1,     1], type =    f16, \n",
      "\n",
      "llama_tensor_get_type : tensor cols 5504 x 2048 are not divisible by 256, required for q6_K - using fallback quantization q8_0\n",
      "converting to q8_0 .. size =    21.50 MiB ->    11.42 MiB\n",
      "[ 212/ 219]                blk.9.ffn_gate.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 213/ 219]                  blk.9.ffn_up.weight - [ 2048,  5504,     1,     1], type =    f16, converting to q5_K .. size =    21.50 MiB ->     7.39 MiB\n",
      "[ 214/ 219]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "[ 215/ 219]                  blk.9.attn_k.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 216/ 219]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 217/ 219]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 218/ 219]                  blk.9.attn_v.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 219/ 219]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n",
      "llama_model_quantize_internal: model size  =  2568.38 MB\n",
      "llama_model_quantize_internal: quant size  =   954.31 MB\n",
      "llama_model_quantize_internal: WARNING: 24 of 170 tensor(s) required fallback quantization\n",
      "\n",
      "main: quantize time = 29242.14 ms\n",
      "main:    total time = 29242.14 ms\n"
     ]
    }
   ],
   "source": [
    "QUANTIZATION_METHODS = [\"q5_k_m\"]\n",
    "\n",
    "for method in QUANTIZATION_METHODS:\n",
    "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
    "    !./llama.cpp/quantize {fp16} {qtype} {method}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your prompt:  # Split data into train and test sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Initialize and train a machine learning model (e.g., Random Forest)\n",
      "Name of the model (options: deepseek-coder-1.3b-python-peft.Q5_K_M.gguf):  deepseek-coder-1.3b-python-peft.Q5_K_M.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 2770 (952d03db)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: seed  = 1714466025\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 219 tensors from deepseek-coder-1.3b-python-peft/deepseek-coder-1.3b-python-peft.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 5504\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 100000.000000\n",
      "llama_model_loader: - kv  12:                    llama.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  13:                  llama.rope.scaling.factor f32              = 4.000000\n",
      "llama_model_loader: - kv  14:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32256]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32256]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32256]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,31757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.bos_token_id u32              = 32013\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 32014\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 32014\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  24:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type q5_1:   12 tensors\n",
      "llama_model_loader: - type q8_0:   12 tensors\n",
      "llama_model_loader: - type q5_K:  133 tensors\n",
      "llama_model_loader: - type q6_K:   13 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: mismatch in special tokens definition ( 243/32256 vs 256/32256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 32256\n",
      "llm_load_print_meta: n_merges         = 31757\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 2048\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 2048\n",
      "llm_load_print_meta: n_embd_v_gqa     = 2048\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 5504\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 100000.0\n",
      "llm_load_print_meta: freq_scale_train = 0.25\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 1.35 B\n",
      "llm_load_print_meta: model size       = 954.31 MiB (5.95 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 32013 '<｜begin▁of▁sentence｜>'\n",
      "llm_load_print_meta: EOS token        = 32014 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: PAD token        = 32014 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: LF token         = 126 'Ä'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA RTX A5000, compute capability 8.6, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 24 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 25/25 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    43.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =   911.00 MiB\n",
      "..........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 100000.0\n",
      "llama_new_context_with_model: freq_scale = 0.25\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    96.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   96.00 MiB, K (f16):   48.00 MiB, V (f16):   48.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =    67.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     5.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 774\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "\n",
      "system_info: n_threads = 48 / 96 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 512, n_batch = 2048, n_predict = 128, n_keep = 1\n",
      "\n",
      "\n",
      "\u001b[33m<｜begin▁of▁sentence｜># Split data into train and test sets from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Initialize and train a machine learning model (e.g., Random Forest)\u001b[0m y_pred = model.predict(X_test)  # Evaluate the model<｜end▁of▁sentence｜> [end of text]\n",
      "\n",
      "llama_print_timings:        load time =     479.10 ms\n",
      "llama_print_timings:      sample time =       0.47 ms /    20 runs   (    0.02 ms per token, 42643.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =      15.89 ms /    84 tokens (    0.19 ms per token,  5286.01 tokens per second)\n",
      "llama_print_timings:        eval time =      72.86 ms /    19 runs   (    3.83 ms per token,   260.76 tokens per second)\n",
      "llama_print_timings:       total time =      95.19 ms /   103 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n",
    "\n",
    "prompt = input(\"Enter your prompt: \")\n",
    "chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n",
    "\n",
    "# Verify the chosen method is in the list\n",
    "if chosen_method not in model_list:\n",
    "    print(\"Invalid name\")\n",
    "else:\n",
    "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
    "    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9a557fdd044151b1a37d1823e8b549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "deepseek-coder-1.3b-python-peft.Q5_K_M.gguf:   0%|          | 0.00/1.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/MadMarx37/deepseek-coder-1.3b-python-peft-GGUF/commit/870609b1eaa1cb6924bb9e2a61eb1ff4ef0c2aab', commit_message='Upload folder using huggingface_hub', commit_description='', oid='870609b1eaa1cb6924bb9e2a61eb1ff4ef0c2aab', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install -q huggingface_hub\n",
    "from huggingface_hub import create_repo, HfApi\n",
    "import getpass\n",
    "\n",
    "hf_token = getpass.getpass()\n",
    "\n",
    "api = HfApi()\n",
    "username = \"MadMarx37\"\n",
    "\n",
    "# Create empty repo\n",
    "create_repo(\n",
    "    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n",
    "    repo_type=\"model\",\n",
    "    exist_ok=True,\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "# Upload gguf files\n",
    "api.upload_folder(\n",
    "    folder_path=MODEL_NAME,\n",
    "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
    "    allow_patterns=f\"*.gguf\",\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
